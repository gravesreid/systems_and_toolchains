{"cells":[{"cell_type":"markdown","id":"123394d2","metadata":{},"source":["<center><h2>SparkML on the cloud</h2></center>"]},{"cell_type":"markdown","id":"85e606a2","metadata":{},"source":["# 1. Running the NSLKDD Example on the Cloud\n","\n","As usual, we start with the data ingestion and data engineering preprocessing. "]},{"cell_type":"code","execution_count":2,"id":"19379aba","metadata":{},"outputs":[],"source":["import pyspark\n","from pyspark.sql import SparkSession, SQLContext\n","from pyspark.ml import Pipeline,Transformer\n","from pyspark.ml.feature import Imputer,StandardScaler,StringIndexer,OneHotEncoder, VectorAssembler\n","\n","from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","import numpy as np\n","\n","col_names = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n","\"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n","\"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n","\"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n","\"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n","\"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n","\"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n","\"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n","\"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n","\"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"class\",\"difficulty\"]\n","\n","nominal_cols = ['protocol_type','service','flag']\n","binary_cols = ['land', 'logged_in', 'root_shell', 'su_attempted', 'is_host_login',\n","'is_guest_login']\n","continuous_cols = ['duration' ,'src_bytes', 'dst_bytes', 'wrong_fragment' ,'urgent', 'hot',\n","'num_failed_logins', 'num_compromised', 'num_root' ,'num_file_creations',\n","'num_shells', 'num_access_files', 'num_outbound_cmds', 'count' ,'srv_count',\n","'serror_rate', 'srv_serror_rate' ,'rerror_rate' ,'srv_rerror_rate',\n","'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate' ,'dst_host_count',\n","'dst_host_srv_count' ,'dst_host_same_srv_rate' ,'dst_host_diff_srv_rate',\n","'dst_host_same_src_port_rate' ,'dst_host_srv_diff_host_rate',\n","'dst_host_serror_rate' ,'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n","'dst_host_srv_rerror_rate']\n","\n","class OutcomeCreater(Transformer): # this defines a transformer that creates the outcome column\n","    \n","    def __init__(self):\n","        super().__init__()\n","\n","    def _transform(self, dataset):\n","        label_to_binary = udf(lambda name: 0.0 if name == 'normal' else 1.0)\n","        output_df = dataset.withColumn('outcome', label_to_binary(col('class'))).drop(\"class\")  \n","        output_df = output_df.withColumn('outcome', col('outcome').cast(DoubleType()))\n","        output_df = output_df.drop('difficulty')\n","        return output_df\n","\n","class FeatureTypeCaster(Transformer): # this transformer will cast the columns as appropriate types  \n","    def __init__(self):\n","        super().__init__()\n","\n","    def _transform(self, dataset):\n","        output_df = dataset\n","        for col_name in binary_cols + continuous_cols:\n","            output_df = output_df.withColumn(col_name,col(col_name).cast(DoubleType()))\n","\n","        return output_df\n","class ColumnDropper(Transformer): # this transformer drops unnecessary columns\n","    def __init__(self, columns_to_drop = None):\n","        super().__init__()\n","        self.columns_to_drop=columns_to_drop\n","    def _transform(self, dataset):\n","        output_df = dataset\n","        for col_name in self.columns_to_drop:\n","            output_df = output_df.drop(col_name)\n","        return output_df\n","\n","def get_preprocess_pipeline():\n","    # Stage where columns are casted as appropriate types\n","    stage_typecaster = FeatureTypeCaster()\n","\n","    # Stage where nominal columns are transformed to index columns using StringIndexer\n","    nominal_id_cols = [x+\"_index\" for x in nominal_cols]\n","    nominal_onehot_cols = [x+\"_encoded\" for x in nominal_cols]\n","    stage_nominal_indexer = StringIndexer(inputCols = nominal_cols, outputCols = nominal_id_cols )\n","\n","    # Stage where the index columns are further transformed using OneHotEncoder\n","    stage_nominal_onehot_encoder = OneHotEncoder(inputCols=nominal_id_cols, outputCols=nominal_onehot_cols)\n","\n","    # Stage where all relevant features are assembled into a vector (and dropping a few)\n","    feature_cols = continuous_cols+binary_cols+nominal_onehot_cols\n","    corelated_cols_to_remove = [\"dst_host_serror_rate\",\"srv_serror_rate\",\"dst_host_srv_serror_rate\",\n","                     \"srv_rerror_rate\",\"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\"]\n","    for col_name in corelated_cols_to_remove:\n","        feature_cols.remove(col_name)\n","    stage_vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"vectorized_features\")\n","\n","    # Stage where we scale the columns\n","    stage_scaler = StandardScaler(inputCol= 'vectorized_features', outputCol= 'features')\n","    \n","\n","    # Stage for creating the outcome column representing whether there is attack \n","    stage_outcome = OutcomeCreater()\n","\n","    # Removing all unnecessary columbs, only keeping the 'features' and 'outcome' columns\n","    stage_column_dropper = ColumnDropper(columns_to_drop = nominal_cols+nominal_id_cols+\n","        nominal_onehot_cols+ binary_cols + continuous_cols + ['vectorized_features'])\n","    # Connect the columns into a pipeline\n","    pipeline = Pipeline(stages=[stage_typecaster,stage_nominal_indexer,stage_nominal_onehot_encoder,\n","        stage_vector_assembler,stage_scaler,stage_outcome,stage_column_dropper])\n","    return pipeline "]},{"cell_type":"code","execution_count":1,"id":"176120f8","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: wget in /home/reid/anaconda3/envs/systems_and_toolchains/lib/python3.12/site-packages (3.2)\n","100% [......................................................] 3441513 / 3441513\n","Saved under KDDTest+ (1).txt\n","/bin/bash: hadoop: command not found\n","100% [....................................................] 19109424 / 19109424\n","Saved under KDDTrain+ (1).txt\n","/bin/bash: hadoop: command not found\n"]}],"source":["# Put the training and test data in the cluster. Uncomment this code if you haven't run this before. \n","\n","!pip install wget \n","!python -m wget https://www.andrew.cmu.edu/user/mfarag/14813/KDDTest+.txt\n","!hadoop fs -put KDDTest+.txt / \n","!python -m wget https://www.andrew.cmu.edu/user/mfarag/14813/KDDTrain+.txt\n","!hadoop fs -put KDDTrain+.txt /\n","    "]},{"cell_type":"markdown","id":"1e6afa37","metadata":{},"source":["## Set up spark to run in cluster mode\n","\n","We are running the notebook on a DataProc cluster, which is designed to run spark on the cluster with multiple worker nodes. \n","\n","To run spark on cluster, when creating the SparkSession, set the master as \"yarn\". (yarn is a type of cluster management tool that DataProc is using). In this cluster mode, the master node will serve as the \"driver\" that runs this notebook. However, each time we have a dataframe operation (e.g. the fit when training an ML model), spark will split the operation into stages, and each stages into tasks, and distribute the tasks to the worker nodes who will run the tasks in parallel. \n","\n","In comparison, if the master is set as \"local\", then all the computation will happen locally on the master node (worker node will not be utilized). \n","\n"]},{"cell_type":"code","execution_count":3,"id":"597f901d","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["24/10/02 16:24:42 WARN Utils: Your hostname, omen-20 resolves to a loopback address: 127.0.1.1; using 192.168.1.156 instead (on interface wlo1)\n","24/10/02 16:24:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n","Exception in thread \"main\" org.apache.spark.SparkException: When running with master 'yarn' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.\n","\tat org.apache.spark.deploy.SparkSubmitArguments.error(SparkSubmitArguments.scala:650)\n","\tat org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments(SparkSubmitArguments.scala:281)\n","\tat org.apache.spark.deploy.SparkSubmitArguments.validateArguments(SparkSubmitArguments.scala:237)\n","\tat org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:122)\n","\tat org.apache.spark.deploy.SparkSubmit$$anon$2$$anon$3.<init>(SparkSubmit.scala:1094)\n","\tat org.apache.spark.deploy.SparkSubmit$$anon$2.parseArguments(SparkSubmit.scala:1094)\n","\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)\n","\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1111)\n","\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1120)\n","\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n"]},{"ename":"RuntimeError","evalue":"Java gateway process exited before sending its port number","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# If you want to run the spark in cluster in the dataproc cluster, set the master as yarn\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# If you want to run locally, set the master as local\u001b[39;00m\n\u001b[1;32m      4\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myarn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSparkML-yarn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m nslkdd_raw \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/KDDTrain+.txt\u001b[39m\u001b[38;5;124m'\u001b[39m,header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mtoDF(\u001b[38;5;241m*\u001b[39mcol_names)\n\u001b[1;32m     10\u001b[0m nslkdd_test_raw \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/KDDTest+.txt\u001b[39m\u001b[38;5;124m'\u001b[39m,header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mtoDF(\u001b[38;5;241m*\u001b[39mcol_names)\n","File \u001b[0;32m~/anaconda3/envs/systems_and_toolchains/lib/python3.12/site-packages/pyspark/sql/session.py:477\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    475\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    476\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 477\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    480\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n","File \u001b[0;32m~/anaconda3/envs/systems_and_toolchains/lib/python3.12/site-packages/pyspark/context.py:512\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 512\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n","File \u001b[0;32m~/anaconda3/envs/systems_and_toolchains/lib/python3.12/site-packages/pyspark/context.py:198\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    196\u001b[0m     )\n\u001b[0;32m--> 198\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    201\u001b[0m         master,\n\u001b[1;32m    202\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    212\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    213\u001b[0m     )\n","File \u001b[0;32m~/anaconda3/envs/systems_and_toolchains/lib/python3.12/site-packages/pyspark/context.py:432\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 432\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n","File \u001b[0;32m~/anaconda3/envs/systems_and_toolchains/lib/python3.12/site-packages/pyspark/java_gateway.py:106\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJava gateway process exited before sending its port number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[1;32m    109\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n","\u001b[0;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"]}],"source":["# If you want to run the spark in cluster in the dataproc cluster, set the master as yarn\n","# If you want to run locally, set the master as local\n","\n","spark = SparkSession.builder \\\n","    .master(\"yarn\") \\\n","    .appName(\"SparkML-yarn\") \\\n","    .getOrCreate()\n","\n","nslkdd_raw = spark.read.csv('/KDDTrain+.txt',header=False).toDF(*col_names)\n","nslkdd_test_raw = spark.read.csv('/KDDTest+.txt',header=False).toDF(*col_names)\n","\n","preprocess_pipeline = get_preprocess_pipeline()\n","preprocess_pipeline_model = preprocess_pipeline.fit(nslkdd_raw)\n","\n","nslkdd_df = preprocess_pipeline_model.transform(nslkdd_raw)\n","nslkdd_df_test = preprocess_pipeline_model.transform(nslkdd_test_raw)\n","\n","\n","nslkdd_df.cache()\n","nslkdd_df_test.cache()\n"]},{"cell_type":"markdown","id":"82b0ce80","metadata":{},"source":["In cluster node, each dataframe is stored in a distributed manner across worker nodes. We can check out how many partition a dataframe has and how many rows each partition is allocated. "]},{"cell_type":"code","execution_count":58,"id":"9915ca22","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of partitions: 2\n"]}],"source":["# Checking how many partitions the dataframe is split into\n","num_partitions = nslkdd_df.rdd.getNumPartitions()\n","print(f\"Number of partitions: {num_partitions}\")\n","\n","\n","# # Uncomment to check how many rows each partition has\n","# def show_partitions(index, iterator):\n","#     yield index, list(iterator)\n","\n","    \n","# # Count how many rows each partition has\n","# partitions_data = nslkdd_df.rdd.mapPartitionsWithIndex(show_partitions).collect()\n","# for partition, data in partitions_data:\n","#     print(f\"Partition {partition}: contains {len(data)} rows\")"]},{"cell_type":"code","execution_count":null,"id":"8649e569","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["24/09/28 21:04:06 WARN DAGScheduler: Broadcasting large task binary with size 1233.3 KiB\n","24/09/28 21:04:29 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n","[Stage 23:=============================>                            (1 + 1) / 2]\r"]}],"source":["from pyspark.ml.classification import RandomForestClassifier\n","\n","rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'outcome',numTrees=500)\n","rf_model = rf.fit(nslkdd_df)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"96ae7848","metadata":{},"outputs":[],"source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","\n","evaluator = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction', \n","    labelCol='outcome', metricName='areaUnderROC')\n","\n","\n","rf_prediction_train = rf_model.transform(nslkdd_df)\n","rf_prediction_test = rf_model.transform(nslkdd_df_test)\n","\n","rf_accuracy_train = (rf_prediction_train.filter(rf_prediction_train.outcome == rf_prediction_train.prediction)\n","    .count()/ float(rf_prediction_train.count()))\n","rf_accuracy_test = (rf_prediction_test.filter(rf_prediction_test.outcome == rf_prediction_test.prediction)\n","    .count() / float(rf_prediction_test.count()))\n","\n","rf_auc = evaluator.evaluate(rf_prediction_test)\n","\n","print(f\"Train accuracy = {np.round(rf_accuracy_train*100,2)}%, test accuracy = {np.round(rf_accuracy_test*100,2)}%, AUC = {np.round(rf_auc,2)}\")"]},{"cell_type":"code","execution_count":null,"id":"a5892b34","metadata":{},"outputs":[],"source":["spark.stop()"]},{"cell_type":"markdown","id":"8aef9fe7","metadata":{},"source":["# 2. Basics about Spark RDD"]},{"cell_type":"markdown","id":"748db058","metadata":{},"source":["In Apache Spark, while DataFrame provide a higher-level abstraction for data, they are fundamentally built on top of RDDs (Resilient Distributed Datasets). Every DataFrame operation eventually gets translated into RDD transformations and actions. "]},{"cell_type":"code","execution_count":49,"id":"dc551302","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["24/09/28 20:58:42 INFO SparkEnv: Registering MapOutputTracker\n","24/09/28 20:58:42 INFO SparkEnv: Registering BlockManagerMaster\n","24/09/28 20:58:42 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n","24/09/28 20:58:42 INFO SparkEnv: Registering OutputCommitCoordinator\n"]},{"name":"stdout","output_type":"stream","text":["Number of partitions: 2\n"]}],"source":["spark = SparkSession.builder \\\n","    .master(\"yarn\") \\\n","    .appName(\"SparkML-RDD-basics\") \\\n","    .getOrCreate()\n","sc = spark.sparkContext\n","\n","# create an RDD and store it distirbutedly. \n","rdd = sc.parallelize([1, 2, 3, 4])\n","\n","# check out haw many partitions the RDD ahs\n","num_partitions = rdd.getNumPartitions()\n","print(f\"Number of partitions: {num_partitions}\")\n"]},{"cell_type":"markdown","id":"70257b36","metadata":{},"source":["## RDD transformations\n","\n","When you perform operations on a DataFrame (like select, filter, groupBy, etc.) or conduct machine learning tasks (fit for a ML model), Spark builds a logical execution plan. This logical plan is then optimized into a physical execution plan, which is composed of RDD transformations and actions.\n","\n","RDD transformations and actions are the most basic operations on RDDs and importantly, RDD transformations/actions can be implemented in a distributed manner across nodes. \n","\n","There are dozens of types of transformations/actions, and the most basic ones are **map, reduce, and filter**. "]},{"cell_type":"markdown","id":"384f1d91","metadata":{},"source":["## Map \n","\n","Map transformation applies a function to each element of RDD and returns a new RDD. \n","\n","As each RDD is partitioned and stored on different nodes, the Map is implemented in parallel on different partitions. "]},{"cell_type":"code","execution_count":50,"id":"900368ca","metadata":{},"outputs":[],"source":["# suppose let's apply a square function \"lambda x: x * x\" to each element of the RDD. \n","squared_rdd = rdd.map(lambda x: x * x)"]},{"cell_type":"code","execution_count":51,"id":"9213370d","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 0:>                                                          (0 + 2) / 2]\r"]},{"name":"stdout","output_type":"stream","text":["[1, 4, 9, 16]\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# Here collect means collect all the partitions into the master node \n","# (which is needed as we want to print the content of the RDD).\n","# Normally this shouldn't be done if the dataset is very large, in which case collect() would crash the driver node. \n","print(squared_rdd.collect())"]},{"cell_type":"markdown","id":"1ca93b01","metadata":{},"source":["## Reduce \n","\n","The reduce() action aggregates elements of an RDD using a binary function (a function that takes two arguments and output one argument). It is typically used to combine all elements into a single result (such as computing a sum, product, or another aggregated value).\n","\n","Similar to Map, Reduce is implemented in parallel on different partitions. "]},{"cell_type":"code","execution_count":52,"id":"51ea90b4","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["24\n"]}],"source":["# let's calculate the product of all elements in the RDD\n","product_result = rdd.reduce(lambda a, b: a * b)\n","print(product_result)"]},{"cell_type":"markdown","id":"4c6ea6bc","metadata":{},"source":["## Filter \n","\n","The filter() transformation returns a new RDD that contains only the elements that satisfy a given condition. It is used to remove elements that donâ€™t meet the criteria.\n","\n"]},{"cell_type":"code","execution_count":53,"id":"b0b464a3","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[2, 4]\n"]}],"source":["# Let's filter and keep all the EVEN numbers in the RDD\n","filtered_rdd = rdd.filter(lambda x: x % 2 == 0)\n","print(filtered_rdd.collect())"]},{"cell_type":"code","execution_count":54,"id":"daca8f27","metadata":{},"outputs":[],"source":["spark.stop()"]},{"cell_type":"code","execution_count":null,"id":"59ead5d0","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"}},"nbformat":4,"nbformat_minor":5}
