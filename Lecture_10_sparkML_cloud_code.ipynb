{"cells":[{"cell_type":"markdown","id":"123394d2","metadata":{},"source":["<center><h2>SparkML on the cloud</h2></center>"]},{"cell_type":"markdown","id":"85e606a2","metadata":{},"source":["# 1. Running the NSLKDD Example on the Cloud\n","\n","As usual, we start with the data ingestion and data engineering preprocessing. "]},{"cell_type":"code","execution_count":55,"id":"19379aba","metadata":{},"outputs":[],"source":["import pyspark\n","from pyspark.sql import SparkSession, SQLContext\n","from pyspark.ml import Pipeline,Transformer\n","from pyspark.ml.feature import Imputer,StandardScaler,StringIndexer,OneHotEncoder, VectorAssembler\n","\n","from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","import numpy as np\n","\n","col_names = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n","\"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n","\"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n","\"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n","\"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n","\"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n","\"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n","\"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n","\"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n","\"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"class\",\"difficulty\"]\n","\n","nominal_cols = ['protocol_type','service','flag']\n","binary_cols = ['land', 'logged_in', 'root_shell', 'su_attempted', 'is_host_login',\n","'is_guest_login']\n","continuous_cols = ['duration' ,'src_bytes', 'dst_bytes', 'wrong_fragment' ,'urgent', 'hot',\n","'num_failed_logins', 'num_compromised', 'num_root' ,'num_file_creations',\n","'num_shells', 'num_access_files', 'num_outbound_cmds', 'count' ,'srv_count',\n","'serror_rate', 'srv_serror_rate' ,'rerror_rate' ,'srv_rerror_rate',\n","'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate' ,'dst_host_count',\n","'dst_host_srv_count' ,'dst_host_same_srv_rate' ,'dst_host_diff_srv_rate',\n","'dst_host_same_src_port_rate' ,'dst_host_srv_diff_host_rate',\n","'dst_host_serror_rate' ,'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n","'dst_host_srv_rerror_rate']\n","\n","class OutcomeCreater(Transformer): # this defines a transformer that creates the outcome column\n","    \n","    def __init__(self):\n","        super().__init__()\n","\n","    def _transform(self, dataset):\n","        label_to_binary = udf(lambda name: 0.0 if name == 'normal' else 1.0)\n","        output_df = dataset.withColumn('outcome', label_to_binary(col('class'))).drop(\"class\")  \n","        output_df = output_df.withColumn('outcome', col('outcome').cast(DoubleType()))\n","        output_df = output_df.drop('difficulty')\n","        return output_df\n","\n","class FeatureTypeCaster(Transformer): # this transformer will cast the columns as appropriate types  \n","    def __init__(self):\n","        super().__init__()\n","\n","    def _transform(self, dataset):\n","        output_df = dataset\n","        for col_name in binary_cols + continuous_cols:\n","            output_df = output_df.withColumn(col_name,col(col_name).cast(DoubleType()))\n","\n","        return output_df\n","class ColumnDropper(Transformer): # this transformer drops unnecessary columns\n","    def __init__(self, columns_to_drop = None):\n","        super().__init__()\n","        self.columns_to_drop=columns_to_drop\n","    def _transform(self, dataset):\n","        output_df = dataset\n","        for col_name in self.columns_to_drop:\n","            output_df = output_df.drop(col_name)\n","        return output_df\n","\n","def get_preprocess_pipeline():\n","    # Stage where columns are casted as appropriate types\n","    stage_typecaster = FeatureTypeCaster()\n","\n","    # Stage where nominal columns are transformed to index columns using StringIndexer\n","    nominal_id_cols = [x+\"_index\" for x in nominal_cols]\n","    nominal_onehot_cols = [x+\"_encoded\" for x in nominal_cols]\n","    stage_nominal_indexer = StringIndexer(inputCols = nominal_cols, outputCols = nominal_id_cols )\n","\n","    # Stage where the index columns are further transformed using OneHotEncoder\n","    stage_nominal_onehot_encoder = OneHotEncoder(inputCols=nominal_id_cols, outputCols=nominal_onehot_cols)\n","\n","    # Stage where all relevant features are assembled into a vector (and dropping a few)\n","    feature_cols = continuous_cols+binary_cols+nominal_onehot_cols\n","    corelated_cols_to_remove = [\"dst_host_serror_rate\",\"srv_serror_rate\",\"dst_host_srv_serror_rate\",\n","                     \"srv_rerror_rate\",\"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\"]\n","    for col_name in corelated_cols_to_remove:\n","        feature_cols.remove(col_name)\n","    stage_vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"vectorized_features\")\n","\n","    # Stage where we scale the columns\n","    stage_scaler = StandardScaler(inputCol= 'vectorized_features', outputCol= 'features')\n","    \n","\n","    # Stage for creating the outcome column representing whether there is attack \n","    stage_outcome = OutcomeCreater()\n","\n","    # Removing all unnecessary columbs, only keeping the 'features' and 'outcome' columns\n","    stage_column_dropper = ColumnDropper(columns_to_drop = nominal_cols+nominal_id_cols+\n","        nominal_onehot_cols+ binary_cols + continuous_cols + ['vectorized_features'])\n","    # Connect the columns into a pipeline\n","    pipeline = Pipeline(stages=[stage_typecaster,stage_nominal_indexer,stage_nominal_onehot_encoder,\n","        stage_vector_assembler,stage_scaler,stage_outcome,stage_column_dropper])\n","    return pipeline "]},{"cell_type":"code","execution_count":56,"id":"176120f8","metadata":{},"outputs":[],"source":["# Put the training and test data in the cluster. Uncomment this code if you haven't run this before. \n","\n","# !pip install wget\n","\n","# !python -m wget https://www.andrew.cmu.edu/user/mfarag/14813/KDDTest+.txt\n","# !hadoop fs -put KDDTest+.txt /\n","\n","# !python -m wget https://www.andrew.cmu.edu/user/mfarag/14813/KDDTrain+.txt\n","# !hadoop fs -put KDDTrain+.txt /\n","    "]},{"cell_type":"markdown","id":"1e6afa37","metadata":{},"source":["## Set up spark to run in cluster mode\n","\n","We are running the notebook on a DataProc cluster, which is designed to run spark on the cluster with multiple worker nodes. \n","\n","To run spark on cluster, when creating the SparkSession, set the master as \"yarn\". (yarn is a type of cluster management tool that DataProc is using). In this cluster mode, the master node will serve as the \"driver\" that runs this notebook. However, each time we have a dataframe operation (e.g. the fit when training an ML model), spark will split the operation into stages, and each stages into tasks, and distribute the tasks to the worker nodes who will run the tasks in parallel. \n","\n","In comparison, if the master is set as \"local\", then all the computation will happen locally on the master node (worker node will not be utilized). \n","\n"]},{"cell_type":"code","execution_count":57,"id":"597f901d","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["24/09/28 21:02:42 INFO SparkEnv: Registering MapOutputTracker\n","24/09/28 21:02:42 INFO SparkEnv: Registering BlockManagerMaster\n","24/09/28 21:02:42 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n","24/09/28 21:02:42 INFO SparkEnv: Registering OutputCommitCoordinator\n","                                                                                \r"]},{"data":{"text/plain":["DataFrame[features: vector, outcome: double]"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["# If you want to run the spark in cluster in the dataproc cluster, set the master as yarn\n","# If you want to run locally, set the master as local\n","\n","spark = SparkSession.builder \\\n","    .master(\"yarn\") \\\n","    .appName(\"SparkML-yarn\") \\\n","    .getOrCreate()\n","\n","nslkdd_raw = spark.read.csv('/KDDTrain+.txt',header=False).toDF(*col_names)\n","nslkdd_test_raw = spark.read.csv('/KDDTest+.txt',header=False).toDF(*col_names)\n","\n","preprocess_pipeline = get_preprocess_pipeline()\n","preprocess_pipeline_model = preprocess_pipeline.fit(nslkdd_raw)\n","\n","nslkdd_df = preprocess_pipeline_model.transform(nslkdd_raw)\n","nslkdd_df_test = preprocess_pipeline_model.transform(nslkdd_test_raw)\n","\n","\n","nslkdd_df.cache()\n","nslkdd_df_test.cache()\n"]},{"cell_type":"markdown","id":"82b0ce80","metadata":{},"source":["In cluster node, each dataframe is stored in a distributed manner across worker nodes. We can check out how many partition a dataframe has and how many rows each partition is allocated. "]},{"cell_type":"code","execution_count":58,"id":"9915ca22","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of partitions: 2\n"]}],"source":["# Checking how many partitions the dataframe is split into\n","num_partitions = nslkdd_df.rdd.getNumPartitions()\n","print(f\"Number of partitions: {num_partitions}\")\n","\n","\n","# # Uncomment to check how many rows each partition has\n","# def show_partitions(index, iterator):\n","#     yield index, list(iterator)\n","\n","    \n","# # Count how many rows each partition has\n","# partitions_data = nslkdd_df.rdd.mapPartitionsWithIndex(show_partitions).collect()\n","# for partition, data in partitions_data:\n","#     print(f\"Partition {partition}: contains {len(data)} rows\")"]},{"cell_type":"code","execution_count":null,"id":"8649e569","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["24/09/28 21:04:06 WARN DAGScheduler: Broadcasting large task binary with size 1233.3 KiB\n","24/09/28 21:04:29 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n","[Stage 23:=============================>                            (1 + 1) / 2]\r"]}],"source":["from pyspark.ml.classification import RandomForestClassifier\n","\n","rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'outcome',numTrees=500)\n","rf_model = rf.fit(nslkdd_df)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"96ae7848","metadata":{},"outputs":[],"source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","\n","evaluator = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction', \n","    labelCol='outcome', metricName='areaUnderROC')\n","\n","\n","rf_prediction_train = rf_model.transform(nslkdd_df)\n","rf_prediction_test = rf_model.transform(nslkdd_df_test)\n","\n","rf_accuracy_train = (rf_prediction_train.filter(rf_prediction_train.outcome == rf_prediction_train.prediction)\n","    .count()/ float(rf_prediction_train.count()))\n","rf_accuracy_test = (rf_prediction_test.filter(rf_prediction_test.outcome == rf_prediction_test.prediction)\n","    .count() / float(rf_prediction_test.count()))\n","\n","rf_auc = evaluator.evaluate(rf_prediction_test)\n","\n","print(f\"Train accuracy = {np.round(rf_accuracy_train*100,2)}%, test accuracy = {np.round(rf_accuracy_test*100,2)}%, AUC = {np.round(rf_auc,2)}\")"]},{"cell_type":"code","execution_count":null,"id":"a5892b34","metadata":{},"outputs":[],"source":["spark.stop()"]},{"cell_type":"markdown","id":"8aef9fe7","metadata":{},"source":["# 2. Basics about Spark RDD"]},{"cell_type":"markdown","id":"748db058","metadata":{},"source":["In Apache Spark, while DataFrame provide a higher-level abstraction for data, they are fundamentally built on top of RDDs (Resilient Distributed Datasets). Every DataFrame operation eventually gets translated into RDD transformations and actions. "]},{"cell_type":"code","execution_count":49,"id":"dc551302","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["24/09/28 20:58:42 INFO SparkEnv: Registering MapOutputTracker\n","24/09/28 20:58:42 INFO SparkEnv: Registering BlockManagerMaster\n","24/09/28 20:58:42 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n","24/09/28 20:58:42 INFO SparkEnv: Registering OutputCommitCoordinator\n"]},{"name":"stdout","output_type":"stream","text":["Number of partitions: 2\n"]}],"source":["spark = SparkSession.builder \\\n","    .master(\"yarn\") \\\n","    .appName(\"SparkML-RDD-basics\") \\\n","    .getOrCreate()\n","sc = spark.sparkContext\n","\n","# create an RDD and store it distirbutedly. \n","rdd = sc.parallelize([1, 2, 3, 4])\n","\n","# check out haw many partitions the RDD ahs\n","num_partitions = rdd.getNumPartitions()\n","print(f\"Number of partitions: {num_partitions}\")\n"]},{"cell_type":"markdown","id":"70257b36","metadata":{},"source":["## RDD transformations\n","\n","When you perform operations on a DataFrame (like select, filter, groupBy, etc.) or conduct machine learning tasks (fit for a ML model), Spark builds a logical execution plan. This logical plan is then optimized into a physical execution plan, which is composed of RDD transformations and actions.\n","\n","RDD transformations and actions are the most basic operations on RDDs and importantly, RDD transformations/actions can be implemented in a distributed manner across nodes. \n","\n","There are dozens of types of transformations/actions, and the most basic ones are **map, reduce, and filter**. "]},{"cell_type":"markdown","id":"384f1d91","metadata":{},"source":["## Map \n","\n","Map transformation applies a function to each element of RDD and returns a new RDD. \n","\n","As each RDD is partitioned and stored on different nodes, the Map is implemented in parallel on different partitions. "]},{"cell_type":"code","execution_count":50,"id":"900368ca","metadata":{},"outputs":[],"source":["# suppose let's apply a square function \"lambda x: x * x\" to each element of the RDD. \n","squared_rdd = rdd.map(lambda x: x * x)"]},{"cell_type":"code","execution_count":51,"id":"9213370d","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 0:>                                                          (0 + 2) / 2]\r"]},{"name":"stdout","output_type":"stream","text":["[1, 4, 9, 16]\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# Here collect means collect all the partitions into the master node \n","# (which is needed as we want to print the content of the RDD).\n","# Normally this shouldn't be done if the dataset is very large, in which case collect() would crash the driver node. \n","print(squared_rdd.collect())"]},{"cell_type":"markdown","id":"1ca93b01","metadata":{},"source":["## Reduce \n","\n","The reduce() action aggregates elements of an RDD using a binary function (a function that takes two arguments and output one argument). It is typically used to combine all elements into a single result (such as computing a sum, product, or another aggregated value).\n","\n","Similar to Map, Reduce is implemented in parallel on different partitions. "]},{"cell_type":"code","execution_count":52,"id":"51ea90b4","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["24\n"]}],"source":["# let's calculate the product of all elements in the RDD\n","product_result = rdd.reduce(lambda a, b: a * b)\n","print(product_result)"]},{"cell_type":"markdown","id":"4c6ea6bc","metadata":{},"source":["## Filter \n","\n","The filter() transformation returns a new RDD that contains only the elements that satisfy a given condition. It is used to remove elements that donâ€™t meet the criteria.\n","\n"]},{"cell_type":"code","execution_count":53,"id":"b0b464a3","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[2, 4]\n"]}],"source":["# Let's filter and keep all the EVEN numbers in the RDD\n","filtered_rdd = rdd.filter(lambda x: x % 2 == 0)\n","print(filtered_rdd.collect())"]},{"cell_type":"code","execution_count":54,"id":"daca8f27","metadata":{},"outputs":[],"source":["spark.stop()"]},{"cell_type":"code","execution_count":null,"id":"59ead5d0","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":5}